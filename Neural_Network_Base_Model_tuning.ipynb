{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geebigib/Data-Science-Project/blob/main/Neural_Network_Base_Model_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzxFI3UZFMBJ",
        "outputId": "06e796b4-4a87-4000-951b-c533c53dec9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Cl9RiEXdfTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7df222-2319-4f74-f2e3-48ef0eadde8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 30 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 40 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 71 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 81 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 102 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 112 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 122 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 133 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 134 kB 8.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pyod --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od9i8uI0btzr",
        "outputId": "5b2e0b16-571e-467e-8774-494a49663882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/SWaT_Dataset_v0.zip\n",
            "  inflating: /content/SWaT_Dataset_Attack_v0.csv  \n",
            "  inflating: /content/SWaT_Dataset_Normal_v0.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/SWaT_Dataset_v0.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ef09e9a3-bebc-407a-8492-838438b4e288"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import pyod\n",
        "import seaborn as sns\n",
        "import timeit\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKdqmsEGblq4"
      },
      "source": [
        "Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IO6QxPKoJq0H"
      },
      "outputs": [],
      "source": [
        "swat_normal = pd.read_csv(\"SWaT_Dataset_Normal_v0.csv\", skiprows=1)\n",
        "swat_attack = pd.read_csv(\"SWaT_Dataset_Attack_v0.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ba1cbb77-2b06-4667-91df-75e8da68b68d"
      },
      "outputs": [],
      "source": [
        "swat_normal.columns = swat_normal.columns.str.replace(' ','')\n",
        "swat_attack.columns = swat_attack.columns.str.replace(' ','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dk2m6eZ7fXPO"
      },
      "outputs": [],
      "source": [
        "swat_attack[\"Normal/Attack\"].replace(' ', '', regex=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NKJf4rvncNvR"
      },
      "outputs": [],
      "source": [
        "test_df = swat_attack.set_index('Timestamp')\n",
        "test_df['label'] = np.where(test_df['Normal/Attack'] == 'Attack', 1, 0)\n",
        "\n",
        "test_df = test_df.drop('Normal/Attack', axis=1)\n",
        "\n",
        "train_df = swat_normal.set_index('Timestamp')\n",
        "train_df['label'] = np.where(train_df['Normal/Attack'] == 'Attack', 1, 0)\n",
        "train_df = train_df.drop('Normal/Attack', axis=1)\n",
        "\n",
        "# trim start stage\n",
        "train_df = train_df.iloc[21600:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IA0qJp-NQn_t"
      },
      "outputs": [],
      "source": [
        "train_df.drop(columns = 'label',inplace = True)\n",
        "label = test_df['label'].values\n",
        "test_df.drop(columns = 'label',inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "34ca9ad8-0f02-4764-b864-1b7e9f9fc56d"
      },
      "outputs": [],
      "source": [
        "#normalise data\n",
        "scaler = MinMaxScaler()\n",
        "scaler = scaler.fit(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "574bbe23-9e71-4079-9b3d-a91b353a5364"
      },
      "outputs": [],
      "source": [
        "train_scale = pd.DataFrame(\n",
        "    scaler.transform(train_df),\n",
        "    index = train_df.index,\n",
        "    columns= train_df.columns\n",
        ")\n",
        "\n",
        "test_scale  = pd.DataFrame(\n",
        "    scaler.transform(test_df),\n",
        "    index = test_df.index,\n",
        "    columns= test_df.columns\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlpVnPfNQpdk",
        "outputId": "b36a087b-ff92-43bf-abf9-4e56710d32d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((475200, 51), (449919, 51))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_scale.shape ,test_scale.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRXg_hqqrYP8",
        "outputId": "9a4ee0c0-f6b7-4395-85d6-ee286d6e9601"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['FIT101', 'LIT101', 'MV101', 'P101', 'P102', 'AIT201', 'AIT202',\n",
              "       'AIT203', 'FIT201', 'MV201', 'P201', 'P202', 'P203', 'P204', 'P205',\n",
              "       'P206', 'DPIT301', 'FIT301', 'LIT301', 'MV301', 'MV302', 'MV303',\n",
              "       'MV304', 'P301', 'P302', 'AIT401', 'AIT402', 'FIT401', 'LIT401', 'P401',\n",
              "       'P402', 'P403', 'P404', 'UV401', 'AIT501', 'AIT502', 'AIT503', 'AIT504',\n",
              "       'FIT501', 'FIT502', 'FIT503', 'FIT504', 'P501', 'P502', 'PIT501',\n",
              "       'PIT502', 'PIT503', 'FIT601', 'P601', 'P602', 'P603'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P1 = ['FIT101', 'LIT101', 'MV101', 'P101', 'P102']"
      ],
      "metadata": {
        "id": "3VEPGP-KrbU_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_p1 = train_scale[P1]\n",
        "test_p1 = test_scale[P1]"
      ],
      "metadata": {
        "id": "YsvAWxOS-dGL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci6PSqZjboNE"
      },
      "source": [
        "Model tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100"
      ],
      "metadata": {
        "id": "_vTeL_uH56hy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9SngcuRjuWHS"
      },
      "outputs": [],
      "source": [
        "from  pyod.models.deep_svdd import DeepSVDD\n",
        "model1 = DeepSVDD(preprocessing = False,epochs=epochs, batch_size =64, hidden_neurons= [64, 32], use_ae=True)\n",
        "model2 = DeepSVDD(preprocessing = False,epochs=epochs, batch_size =64, hidden_neurons= [64, 32], use_ae=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(train_scale)\n",
        "model2.fit(train_scale)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzPkM-NEiTTt",
        "outputId": "2e03d47b-0aa5-4a47-9169-21080a77b984"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 51)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 64)           3264        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " net_output (Dense)             (None, 32)           2048        ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.subtract_2 (TFOpLambda  (None, 32)          0           ['net_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.pow_1 (TFOpLambda)     (None, 32)           0           ['tf.math.subtract_2[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum_1 (TFOpLamb  (None,)             0           ['tf.math.pow_1[0][0]']          \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 32)           1024        ['net_output[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 32)           0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 64)           2048        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 64)           0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 51)           3264        ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.subtract_3 (TFOpLambda  (None, 51)          0           ['dense_7[0][0]',                \n",
            " )                                                                'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.square_1 (TFOpLambda)  (None, 51)           0           ['tf.math.subtract_3[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_2 (TFOpLam  ()                  0           ['tf.math.reduce_sum_1[0][0]']   \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_3 (TFOpLam  ()                  0           ['tf.math.square_1[0][0]']       \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  ()                  0           ['tf.math.reduce_mean_2[0][0]',  \n",
            " mbda)                                                            'tf.math.reduce_mean_3[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  ()                  0           ['tf.__operators__.add_2[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_1 (AddLoss)           ()                   0           ['tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,648\n",
            "Trainable params: 11,648\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "6683/6683 [==============================] - 33s 5ms/step - loss: 0.4691 - val_loss: 0.4001\n",
            "Epoch 2/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3973 - val_loss: 0.3882\n",
            "Epoch 3/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3908 - val_loss: 0.3862\n",
            "Epoch 4/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3887 - val_loss: 0.3851\n",
            "Epoch 5/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3876 - val_loss: 0.3844\n",
            "Epoch 6/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3864 - val_loss: 0.3832\n",
            "Epoch 7/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3859 - val_loss: 0.3828\n",
            "Epoch 8/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3855 - val_loss: 0.3854\n",
            "Epoch 9/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3853 - val_loss: 0.3840\n",
            "Epoch 10/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3850 - val_loss: 0.3857\n",
            "Epoch 11/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3850 - val_loss: 0.3878\n",
            "Epoch 12/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3851 - val_loss: 0.3887\n",
            "Epoch 13/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3852 - val_loss: 0.3865\n",
            "Epoch 14/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3851 - val_loss: 0.3938\n",
            "Epoch 15/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3855 - val_loss: 0.3910\n",
            "Epoch 16/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3859 - val_loss: 0.3904\n",
            "Epoch 17/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3866 - val_loss: 0.3874\n",
            "Epoch 18/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3866 - val_loss: 0.3903\n",
            "Epoch 19/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3864 - val_loss: 0.3878\n",
            "Epoch 20/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3861 - val_loss: 0.3909\n",
            "Epoch 21/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3873 - val_loss: 0.3909\n",
            "Epoch 22/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3879 - val_loss: 0.3883\n",
            "Epoch 23/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3877 - val_loss: 0.3892\n",
            "Epoch 24/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3877 - val_loss: 0.3893\n",
            "Epoch 25/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3875 - val_loss: 0.3919\n",
            "Epoch 26/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3878 - val_loss: 0.3944\n",
            "Epoch 27/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3875 - val_loss: 0.3910\n",
            "Epoch 28/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3874 - val_loss: 0.3952\n",
            "Epoch 29/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3875 - val_loss: 0.3883\n",
            "Epoch 30/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3876 - val_loss: 0.3906\n",
            "Epoch 31/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3876 - val_loss: 0.3912\n",
            "Epoch 32/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3876 - val_loss: 0.3922\n",
            "Epoch 33/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3874 - val_loss: 0.3892\n",
            "Epoch 34/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3874 - val_loss: 0.3901\n",
            "Epoch 35/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3874 - val_loss: 0.3927\n",
            "Epoch 36/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3874 - val_loss: 0.3884\n",
            "Epoch 37/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3874 - val_loss: 0.3926\n",
            "Epoch 38/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3873 - val_loss: 0.3915\n",
            "Epoch 39/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3872 - val_loss: 0.3892\n",
            "Epoch 40/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3873 - val_loss: 0.3890\n",
            "Epoch 41/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3874 - val_loss: 0.3996\n",
            "Epoch 42/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3873 - val_loss: 0.3909\n",
            "Epoch 43/100\n",
            "6683/6683 [==============================] - 33s 5ms/step - loss: 0.3872 - val_loss: 0.3875\n",
            "Epoch 44/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3874 - val_loss: 0.3926\n",
            "Epoch 45/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3872 - val_loss: 0.3886\n",
            "Epoch 46/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3873 - val_loss: 0.3881\n",
            "Epoch 47/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3874 - val_loss: 0.3879\n",
            "Epoch 48/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3872 - val_loss: 0.3912\n",
            "Epoch 49/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3872 - val_loss: 0.3875\n",
            "Epoch 50/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3873 - val_loss: 0.3920\n",
            "Epoch 51/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3873 - val_loss: 0.3871\n",
            "Epoch 52/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3872 - val_loss: 0.3928\n",
            "Epoch 53/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3873 - val_loss: 0.3921\n",
            "Epoch 54/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3872 - val_loss: 0.3913\n",
            "Epoch 55/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3872 - val_loss: 0.3878\n",
            "Epoch 56/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3873 - val_loss: 0.3934\n",
            "Epoch 57/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3872 - val_loss: 0.3900\n",
            "Epoch 58/100\n",
            "6683/6683 [==============================] - 33s 5ms/step - loss: 0.3871 - val_loss: 0.3869\n",
            "Epoch 59/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3872 - val_loss: 0.3914\n",
            "Epoch 60/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3873 - val_loss: 0.3892\n",
            "Epoch 61/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3872 - val_loss: 0.3917\n",
            "Epoch 62/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3873 - val_loss: 0.3921\n",
            "Epoch 63/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3870 - val_loss: 0.3924\n",
            "Epoch 64/100\n",
            "6683/6683 [==============================] - 30s 4ms/step - loss: 0.3871 - val_loss: 0.3990\n",
            "Epoch 65/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3871 - val_loss: 0.3929\n",
            "Epoch 66/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3871 - val_loss: 0.3867\n",
            "Epoch 67/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3871 - val_loss: 0.3881\n",
            "Epoch 68/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3871 - val_loss: 0.3885\n",
            "Epoch 69/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3871 - val_loss: 0.3893\n",
            "Epoch 70/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3871 - val_loss: 0.3936\n",
            "Epoch 71/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3871 - val_loss: 0.3922\n",
            "Epoch 72/100\n",
            "6683/6683 [==============================] - 30s 4ms/step - loss: 0.3871 - val_loss: 0.3909\n",
            "Epoch 73/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3871 - val_loss: 0.3871\n",
            "Epoch 74/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3873 - val_loss: 0.3910\n",
            "Epoch 75/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3871 - val_loss: 0.3912\n",
            "Epoch 76/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3870 - val_loss: 0.3950\n",
            "Epoch 77/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3871 - val_loss: 0.3899\n",
            "Epoch 78/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3870 - val_loss: 0.3877\n",
            "Epoch 79/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3871 - val_loss: 0.3903\n",
            "Epoch 80/100\n",
            "6683/6683 [==============================] - 30s 4ms/step - loss: 0.3872 - val_loss: 0.3899\n",
            "Epoch 81/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3871 - val_loss: 0.3916\n",
            "Epoch 82/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3872 - val_loss: 0.3888\n",
            "Epoch 83/100\n",
            "6683/6683 [==============================] - 30s 5ms/step - loss: 0.3871 - val_loss: 0.3885\n",
            "Epoch 84/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3869 - val_loss: 0.3896\n",
            "Epoch 85/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3871 - val_loss: 0.3856\n",
            "Epoch 86/100\n",
            "6683/6683 [==============================] - 33s 5ms/step - loss: 0.3871 - val_loss: 0.3898\n",
            "Epoch 87/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3873 - val_loss: 0.3903\n",
            "Epoch 88/100\n",
            "6683/6683 [==============================] - 33s 5ms/step - loss: 0.3869 - val_loss: 0.3864\n",
            "Epoch 89/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3871 - val_loss: 0.3879\n",
            "Epoch 90/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3871 - val_loss: 0.3869\n",
            "Epoch 91/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3870 - val_loss: 0.3879\n",
            "Epoch 92/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3871 - val_loss: 0.3895\n",
            "Epoch 93/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3871 - val_loss: 0.3882\n",
            "Epoch 94/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3869 - val_loss: 0.3884\n",
            "Epoch 95/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3870 - val_loss: 0.3887\n",
            "Epoch 96/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3870 - val_loss: 0.3863\n",
            "Epoch 97/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3871 - val_loss: 0.3895\n",
            "Epoch 98/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3872 - val_loss: 0.3913\n",
            "Epoch 99/100\n",
            "6683/6683 [==============================] - 32s 5ms/step - loss: 0.3870 - val_loss: 0.3953\n",
            "Epoch 100/100\n",
            "6683/6683 [==============================] - 31s 5ms/step - loss: 0.3869 - val_loss: 0.3869\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 51)]              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                3264      \n",
            "                                                                 \n",
            " net_output (Dense)          (None, 32)                2048      \n",
            "                                                                 \n",
            " tf.math.subtract_5 (TFOpLam  (None, 32)               0         \n",
            " bda)                                                            \n",
            "                                                                 \n",
            " tf.math.pow_3 (TFOpLambda)  (None, 32)                0         \n",
            "                                                                 \n",
            " tf.math.reduce_sum_3 (TFOpL  (None,)                  0         \n",
            " ambda)                                                          \n",
            "                                                                 \n",
            " tf.math.reduce_mean_5 (TFOp  ()                       0         \n",
            " Lambda)                                                         \n",
            "                                                                 \n",
            " tf.__operators__.add_5 (TFO  ()                       0         \n",
            " pLambda)                                                        \n",
            "                                                                 \n",
            " add_loss_3 (AddLoss)        ()                        0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,312\n",
            "Trainable params: 5,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3160 - val_loss: 0.2853\n",
            "Epoch 2/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2844 - val_loss: 0.2842\n",
            "Epoch 3/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2833 - val_loss: 0.2827\n",
            "Epoch 4/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2829 - val_loss: 0.2857\n",
            "Epoch 5/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2826 - val_loss: 0.2830\n",
            "Epoch 6/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2824 - val_loss: 0.2829\n",
            "Epoch 7/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2823 - val_loss: 0.2829\n",
            "Epoch 8/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2822 - val_loss: 0.2819\n",
            "Epoch 9/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2821 - val_loss: 0.2822\n",
            "Epoch 10/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2820 - val_loss: 0.2818\n",
            "Epoch 11/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2820 - val_loss: 0.2814\n",
            "Epoch 12/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2820 - val_loss: 0.2815\n",
            "Epoch 13/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2819 - val_loss: 0.2818\n",
            "Epoch 14/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2819 - val_loss: 0.2821\n",
            "Epoch 15/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2819 - val_loss: 0.2813\n",
            "Epoch 16/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2819 - val_loss: 0.2825\n",
            "Epoch 17/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2819 - val_loss: 0.2827\n",
            "Epoch 18/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2819 - val_loss: 0.2816\n",
            "Epoch 19/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2818 - val_loss: 0.2815\n",
            "Epoch 20/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2818 - val_loss: 0.2812\n",
            "Epoch 21/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2818 - val_loss: 0.2813\n",
            "Epoch 22/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2818 - val_loss: 0.2812\n",
            "Epoch 23/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2818 - val_loss: 0.2812\n",
            "Epoch 24/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2818 - val_loss: 0.2812\n",
            "Epoch 25/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2818 - val_loss: 0.2811\n",
            "Epoch 26/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2818 - val_loss: 0.2817\n",
            "Epoch 27/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2818 - val_loss: 0.2812\n",
            "Epoch 28/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2817 - val_loss: 0.2815\n",
            "Epoch 29/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2817 - val_loss: 0.2812\n",
            "Epoch 30/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2817 - val_loss: 0.2812\n",
            "Epoch 31/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2817 - val_loss: 0.2836\n",
            "Epoch 32/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2817 - val_loss: 0.2811\n",
            "Epoch 33/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2817 - val_loss: 0.2811\n",
            "Epoch 34/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2817 - val_loss: 0.2813\n",
            "Epoch 35/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2817 - val_loss: 0.2832\n",
            "Epoch 36/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2817 - val_loss: 0.2820\n",
            "Epoch 37/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2817 - val_loss: 0.2813\n",
            "Epoch 38/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2816 - val_loss: 0.2812\n",
            "Epoch 39/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2816 - val_loss: 0.2829\n",
            "Epoch 40/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2816 - val_loss: 0.2812\n",
            "Epoch 41/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2816 - val_loss: 0.2811\n",
            "Epoch 42/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2816 - val_loss: 0.2810\n",
            "Epoch 43/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2816 - val_loss: 0.2811\n",
            "Epoch 44/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2816 - val_loss: 0.2836\n",
            "Epoch 45/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2816 - val_loss: 0.2810\n",
            "Epoch 46/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 47/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 48/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2815\n",
            "Epoch 49/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2817\n",
            "Epoch 50/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2813\n",
            "Epoch 51/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2819\n",
            "Epoch 52/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2812\n",
            "Epoch 53/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2811\n",
            "Epoch 54/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 55/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 56/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 57/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 58/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2812\n",
            "Epoch 59/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2812\n",
            "Epoch 60/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 61/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2812\n",
            "Epoch 62/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 63/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2821\n",
            "Epoch 64/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2812\n",
            "Epoch 65/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2815\n",
            "Epoch 66/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 67/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 68/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2809\n",
            "Epoch 69/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2818\n",
            "Epoch 70/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 71/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2811\n",
            "Epoch 72/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 73/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2809\n",
            "Epoch 74/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 75/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 76/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2811\n",
            "Epoch 77/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2813\n",
            "Epoch 78/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 79/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2812\n",
            "Epoch 80/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 81/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2811\n",
            "Epoch 82/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2813\n",
            "Epoch 83/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2811\n",
            "Epoch 84/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 85/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2822\n",
            "Epoch 86/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2811\n",
            "Epoch 87/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 88/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2816\n",
            "Epoch 89/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2809\n",
            "Epoch 90/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 91/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2814\n",
            "Epoch 92/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2809\n",
            "Epoch 93/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 94/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2812\n",
            "Epoch 95/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2815 - val_loss: 0.2816\n",
            "Epoch 96/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 97/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.2815 - val_loss: 0.2811\n",
            "Epoch 98/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2812\n",
            "Epoch 99/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.2815 - val_loss: 0.2810\n",
            "Epoch 100/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.2815 - val_loss: 0.2810\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepSVDD(batch_size=64,\n",
              "     c=array([0.47435, 0.1    , 0.1    , 0.1    , 0.1    , 0.1    , 0.1    ,\n",
              "       0.1    , 0.1    , 0.16553, 0.84873, 0.1    , 0.74385, 0.1    ,\n",
              "       0.     , 0.32848, 0.     , 0.20296, 0.1    , 0.1    , 0.19765,\n",
              "       0.1    , 0.1    , 0.26197, 0.37444, 0.     , 0.91189, 0.1    ,\n",
              "       0.1    , 0.1    , 0.38527, 0.1    ], dtype=float32),\n",
              "     contamination=0.1, dropout_rate=0.2, epochs=100,\n",
              "     hidden_activation='relu', hidden_neurons=[64, 32], l2_regularizer=0.1,\n",
              "     optimizer='adam', output_activation='sigmoid', preprocessing=False,\n",
              "     random_state=None, use_ae=False, validation_size=0.1, verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss1 = model1.history_\n",
        "loss2 = model2.history_"
      ],
      "metadata": {
        "id": "uEO5XDvAD3Z5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(1,101), loss1['val_loss'])\n",
        "plt.plot(np.arange(1,101), loss2['val_loss'])\n",
        "plt.legend(['loss1','loss2'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "EvRejg40GtTf",
        "outputId": "bd45f389-acce-4795-c868-0cbe74731f8f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f45e4150650>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = DeepSVDD(preprocessing = False,epochs=epochs, batch_size =64, hidden_neurons= [128, 64], use_ae=False)\n",
        "model4 = DeepSVDD(preprocessing = False,epochs=epochs, batch_size =128, hidden_neurons= [64, 32], use_ae=False)\n",
        "model5 = DeepSVDD(preprocessing = False,epochs=epochs, batch_size =128, hidden_neurons= [128, 64], use_ae=False)"
      ],
      "metadata": {
        "id": "esOuG30zCkE1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.fit(train_scale)\n",
        "model4.fit(train_scale)\n",
        "model5.fit(train_scale)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBVUOGBKCVh0",
        "outputId": "4d6261ca-39c6-45b5-f82d-bfea4ebd6ccf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 51)]              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 128)               6528      \n",
            "                                                                 \n",
            " net_output (Dense)          (None, 64)                8192      \n",
            "                                                                 \n",
            " tf.math.subtract_7 (TFOpLam  (None, 64)               0         \n",
            " bda)                                                            \n",
            "                                                                 \n",
            " tf.math.pow_5 (TFOpLambda)  (None, 64)                0         \n",
            "                                                                 \n",
            " tf.math.reduce_sum_5 (TFOpL  (None,)                  0         \n",
            " ambda)                                                          \n",
            "                                                                 \n",
            " tf.math.reduce_mean_7 (TFOp  ()                       0         \n",
            " Lambda)                                                         \n",
            "                                                                 \n",
            " tf.__operators__.add_7 (TFO  ()                       0         \n",
            " pLambda)                                                        \n",
            "                                                                 \n",
            " add_loss_5 (AddLoss)        ()                        0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,720\n",
            "Trainable params: 14,720\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3266 - val_loss: 0.3047\n",
            "Epoch 2/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3042 - val_loss: 0.3032\n",
            "Epoch 3/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3035 - val_loss: 0.3028\n",
            "Epoch 4/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3033 - val_loss: 0.3026\n",
            "Epoch 5/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3031 - val_loss: 0.3024\n",
            "Epoch 6/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3031 - val_loss: 0.3030\n",
            "Epoch 7/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3029 - val_loss: 0.3023\n",
            "Epoch 8/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3029 - val_loss: 0.3036\n",
            "Epoch 9/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3028 - val_loss: 0.3025\n",
            "Epoch 10/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3027 - val_loss: 0.3020\n",
            "Epoch 11/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3027 - val_loss: 0.3021\n",
            "Epoch 12/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3026 - val_loss: 0.3021\n",
            "Epoch 13/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3026 - val_loss: 0.3193\n",
            "Epoch 14/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3026 - val_loss: 0.3023\n",
            "Epoch 15/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3025 - val_loss: 0.3019\n",
            "Epoch 16/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3025 - val_loss: 0.3021\n",
            "Epoch 17/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3025 - val_loss: 0.3020\n",
            "Epoch 18/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3024 - val_loss: 0.3018\n",
            "Epoch 19/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3024 - val_loss: 0.3022\n",
            "Epoch 20/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3023 - val_loss: 0.3069\n",
            "Epoch 21/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3023 - val_loss: 0.3017\n",
            "Epoch 22/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3022 - val_loss: 0.3017\n",
            "Epoch 23/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3022 - val_loss: 0.3026\n",
            "Epoch 24/100\n",
            "6683/6683 [==============================] - 23s 4ms/step - loss: 0.3022 - val_loss: 0.3017\n",
            "Epoch 25/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3021 - val_loss: 0.3019\n",
            "Epoch 26/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3021 - val_loss: 0.3017\n",
            "Epoch 27/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3021 - val_loss: 0.3059\n",
            "Epoch 28/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3021 - val_loss: 0.3021\n",
            "Epoch 29/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3021 - val_loss: 0.3015\n",
            "Epoch 30/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3021 - val_loss: 0.3017\n",
            "Epoch 31/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3021 - val_loss: 0.3016\n",
            "Epoch 32/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3020 - val_loss: 0.3031\n",
            "Epoch 33/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3020 - val_loss: 0.3028\n",
            "Epoch 34/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3020 - val_loss: 0.3015\n",
            "Epoch 35/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3021 - val_loss: 0.3015\n",
            "Epoch 36/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3020 - val_loss: 0.3017\n",
            "Epoch 37/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3020 - val_loss: 0.3017\n",
            "Epoch 38/100\n",
            "6683/6683 [==============================] - 24s 4ms/step - loss: 0.3020 - val_loss: 0.3015\n",
            "Epoch 39/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3020 - val_loss: 0.3016\n",
            "Epoch 40/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3020 - val_loss: 0.3014\n",
            "Epoch 41/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3020 - val_loss: 0.3021\n",
            "Epoch 42/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3023\n",
            "Epoch 43/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3075\n",
            "Epoch 44/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 45/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3015\n",
            "Epoch 46/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3035\n",
            "Epoch 47/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 48/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 49/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 50/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 51/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3016\n",
            "Epoch 52/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3015\n",
            "Epoch 53/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 54/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 55/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 56/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 57/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 58/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 59/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3016\n",
            "Epoch 60/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 61/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 62/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3027\n",
            "Epoch 63/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3025\n",
            "Epoch 64/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3018\n",
            "Epoch 65/100\n",
            "6683/6683 [==============================] - 23s 4ms/step - loss: 0.3019 - val_loss: 0.3015\n",
            "Epoch 66/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 67/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3027\n",
            "Epoch 68/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3017\n",
            "Epoch 69/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 70/100\n",
            "6683/6683 [==============================] - 23s 4ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 71/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3015\n",
            "Epoch 72/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 73/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 74/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3015\n",
            "Epoch 75/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 76/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3013\n",
            "Epoch 77/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3013\n",
            "Epoch 78/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 79/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 80/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3014\n",
            "Epoch 81/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3014\n",
            "Epoch 82/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3015\n",
            "Epoch 83/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3019\n",
            "Epoch 84/100\n",
            "6683/6683 [==============================] - 23s 4ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 85/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3013\n",
            "Epoch 86/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3019 - val_loss: 0.3014\n",
            "Epoch 87/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3015\n",
            "Epoch 88/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3018 - val_loss: 0.3013\n",
            "Epoch 89/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3017\n",
            "Epoch 90/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3019 - val_loss: 0.3013\n",
            "Epoch 91/100\n",
            "6683/6683 [==============================] - 23s 3ms/step - loss: 0.3018 - val_loss: 0.3013\n",
            "Epoch 92/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3014\n",
            "Epoch 93/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3013\n",
            "Epoch 94/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3014\n",
            "Epoch 95/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3014\n",
            "Epoch 96/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3014\n",
            "Epoch 97/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3013\n",
            "Epoch 98/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3019\n",
            "Epoch 99/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3013\n",
            "Epoch 100/100\n",
            "6683/6683 [==============================] - 22s 3ms/step - loss: 0.3018 - val_loss: 0.3013\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 51)]              0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                3264      \n",
            "                                                                 \n",
            " net_output (Dense)          (None, 32)                2048      \n",
            "                                                                 \n",
            " tf.math.subtract_9 (TFOpLam  (None, 32)               0         \n",
            " bda)                                                            \n",
            "                                                                 \n",
            " tf.math.pow_7 (TFOpLambda)  (None, 32)                0         \n",
            "                                                                 \n",
            " tf.math.reduce_sum_7 (TFOpL  (None,)                  0         \n",
            " ambda)                                                          \n",
            "                                                                 \n",
            " tf.math.reduce_mean_9 (TFOp  ()                       0         \n",
            " Lambda)                                                         \n",
            "                                                                 \n",
            " tf.__operators__.add_9 (TFO  ()                       0         \n",
            " pLambda)                                                        \n",
            "                                                                 \n",
            " add_loss_7 (AddLoss)        ()                        0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,312\n",
            "Trainable params: 5,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.3089 - val_loss: 0.2476\n",
            "Epoch 2/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2449 - val_loss: 0.2428\n",
            "Epoch 3/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2422 - val_loss: 0.2414\n",
            "Epoch 4/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2413 - val_loss: 0.2410\n",
            "Epoch 5/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2409 - val_loss: 0.2405\n",
            "Epoch 6/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2406 - val_loss: 0.2402\n",
            "Epoch 7/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2405 - val_loss: 0.2402\n",
            "Epoch 8/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2403 - val_loss: 0.2399\n",
            "Epoch 9/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2402 - val_loss: 0.2403\n",
            "Epoch 10/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2402 - val_loss: 0.2398\n",
            "Epoch 11/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2401 - val_loss: 0.2397\n",
            "Epoch 12/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2401 - val_loss: 0.2401\n",
            "Epoch 13/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2400 - val_loss: 0.2396\n",
            "Epoch 14/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2400 - val_loss: 0.2400\n",
            "Epoch 15/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2399 - val_loss: 0.2395\n",
            "Epoch 16/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2399 - val_loss: 0.2395\n",
            "Epoch 17/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2398 - val_loss: 0.2397\n",
            "Epoch 18/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2398 - val_loss: 0.2395\n",
            "Epoch 19/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2398 - val_loss: 0.2394\n",
            "Epoch 20/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2398 - val_loss: 0.2393\n",
            "Epoch 21/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2398 - val_loss: 0.2402\n",
            "Epoch 22/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2397 - val_loss: 0.2399\n",
            "Epoch 23/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2397 - val_loss: 0.2392\n",
            "Epoch 24/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2397 - val_loss: 0.2392\n",
            "Epoch 25/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2397 - val_loss: 0.2393\n",
            "Epoch 26/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2396 - val_loss: 0.2392\n",
            "Epoch 27/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2396 - val_loss: 0.2393\n",
            "Epoch 28/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2396 - val_loss: 0.2403\n",
            "Epoch 29/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2396 - val_loss: 0.2396\n",
            "Epoch 30/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2396 - val_loss: 0.2391\n",
            "Epoch 31/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2396 - val_loss: 0.2391\n",
            "Epoch 32/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2396 - val_loss: 0.2398\n",
            "Epoch 33/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2396 - val_loss: 0.2392\n",
            "Epoch 34/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2400\n",
            "Epoch 35/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2419\n",
            "Epoch 36/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2396 - val_loss: 0.2392\n",
            "Epoch 37/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2396 - val_loss: 0.2391\n",
            "Epoch 38/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 39/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2394\n",
            "Epoch 40/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 41/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 42/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 43/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2396 - val_loss: 0.2390\n",
            "Epoch 44/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2397\n",
            "Epoch 45/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2394\n",
            "Epoch 46/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 47/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2392\n",
            "Epoch 48/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2401\n",
            "Epoch 49/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2401\n",
            "Epoch 50/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 51/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2397\n",
            "Epoch 52/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2392\n",
            "Epoch 53/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 54/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2401\n",
            "Epoch 55/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 56/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 57/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 58/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 59/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 60/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2396\n",
            "Epoch 61/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2396\n",
            "Epoch 62/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2399\n",
            "Epoch 63/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 64/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 65/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 66/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 67/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 68/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 69/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2399\n",
            "Epoch 70/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 71/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2427\n",
            "Epoch 72/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2395\n",
            "Epoch 73/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 74/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2394 - val_loss: 0.2396\n",
            "Epoch 75/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 76/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 77/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 78/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 79/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 80/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2391\n",
            "Epoch 81/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 82/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 83/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 84/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2395 - val_loss: 0.2390\n",
            "Epoch 85/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2394 - val_loss: 0.2389\n",
            "Epoch 86/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 87/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2414\n",
            "Epoch 88/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2433\n",
            "Epoch 89/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 90/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2412\n",
            "Epoch 91/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 92/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 93/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2392\n",
            "Epoch 94/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2394 - val_loss: 0.2389\n",
            "Epoch 95/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2395 - val_loss: 0.2389\n",
            "Epoch 96/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2394 - val_loss: 0.2389\n",
            "Epoch 97/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 98/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2390\n",
            "Epoch 99/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2394 - val_loss: 0.2392\n",
            "Epoch 100/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2394 - val_loss: 0.2391\n",
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 51)]              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 128)               6528      \n",
            "                                                                 \n",
            " net_output (Dense)          (None, 64)                8192      \n",
            "                                                                 \n",
            " tf.math.subtract_11 (TFOpLa  (None, 64)               0         \n",
            " mbda)                                                           \n",
            "                                                                 \n",
            " tf.math.pow_9 (TFOpLambda)  (None, 64)                0         \n",
            "                                                                 \n",
            " tf.math.reduce_sum_9 (TFOpL  (None,)                  0         \n",
            " ambda)                                                          \n",
            "                                                                 \n",
            " tf.math.reduce_mean_11 (TFO  ()                       0         \n",
            " pLambda)                                                        \n",
            "                                                                 \n",
            " tf.__operators__.add_11 (TF  ()                       0         \n",
            " OpLambda)                                                       \n",
            "                                                                 \n",
            " add_loss_9 (AddLoss)        ()                        0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,720\n",
            "Trainable params: 14,720\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "3342/3342 [==============================] - 13s 4ms/step - loss: 0.2490 - val_loss: 0.2114\n",
            "Epoch 2/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2105 - val_loss: 0.2096\n",
            "Epoch 3/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2096 - val_loss: 0.2091\n",
            "Epoch 4/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2092 - val_loss: 0.2092\n",
            "Epoch 5/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2090 - val_loss: 0.2085\n",
            "Epoch 6/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2088 - val_loss: 0.2088\n",
            "Epoch 7/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2087 - val_loss: 0.2085\n",
            "Epoch 8/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2086 - val_loss: 0.2082\n",
            "Epoch 9/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2086 - val_loss: 0.2089\n",
            "Epoch 10/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2086 - val_loss: 0.2081\n",
            "Epoch 11/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2085 - val_loss: 0.2091\n",
            "Epoch 12/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2085 - val_loss: 0.2083\n",
            "Epoch 13/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2085 - val_loss: 0.2080\n",
            "Epoch 14/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2084 - val_loss: 0.2082\n",
            "Epoch 15/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2084 - val_loss: 0.2080\n",
            "Epoch 16/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2083 - val_loss: 0.2099\n",
            "Epoch 17/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2083 - val_loss: 0.2086\n",
            "Epoch 18/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2083 - val_loss: 0.2105\n",
            "Epoch 19/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2083 - val_loss: 0.2080\n",
            "Epoch 20/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2083 - val_loss: 0.2087\n",
            "Epoch 21/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2083 - val_loss: 0.2080\n",
            "Epoch 22/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2082 - val_loss: 0.2124\n",
            "Epoch 23/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2082 - val_loss: 0.2091\n",
            "Epoch 24/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2082 - val_loss: 0.2078\n",
            "Epoch 25/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2081 - val_loss: 0.2078\n",
            "Epoch 26/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2081 - val_loss: 0.2078\n",
            "Epoch 27/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2081 - val_loss: 0.2077\n",
            "Epoch 28/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2081 - val_loss: 0.2086\n",
            "Epoch 29/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2081 - val_loss: 0.2077\n",
            "Epoch 30/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2081 - val_loss: 0.2077\n",
            "Epoch 31/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2081 - val_loss: 0.2088\n",
            "Epoch 32/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2081 - val_loss: 0.2078\n",
            "Epoch 33/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2081 - val_loss: 0.2083\n",
            "Epoch 34/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2081 - val_loss: 0.2078\n",
            "Epoch 35/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2080 - val_loss: 0.2077\n",
            "Epoch 36/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2080 - val_loss: 0.2077\n",
            "Epoch 37/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2080 - val_loss: 0.2076\n",
            "Epoch 38/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2080 - val_loss: 0.2078\n",
            "Epoch 39/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2080 - val_loss: 0.2076\n",
            "Epoch 40/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2080 - val_loss: 0.2076\n",
            "Epoch 41/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2080 - val_loss: 0.2077\n",
            "Epoch 42/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2080 - val_loss: 0.2082\n",
            "Epoch 43/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2078\n",
            "Epoch 44/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2080 - val_loss: 0.2088\n",
            "Epoch 45/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2080 - val_loss: 0.2076\n",
            "Epoch 46/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2079\n",
            "Epoch 47/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2080 - val_loss: 0.2083\n",
            "Epoch 48/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 49/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2078\n",
            "Epoch 50/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 51/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2080 - val_loss: 0.2085\n",
            "Epoch 52/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 53/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2082\n",
            "Epoch 54/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 55/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 56/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 57/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 58/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 59/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2077\n",
            "Epoch 60/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2081\n",
            "Epoch 61/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 62/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 63/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2107\n",
            "Epoch 64/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 65/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 66/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 67/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2078\n",
            "Epoch 68/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 69/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2111\n",
            "Epoch 70/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 71/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 72/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 73/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 74/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 75/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 76/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2077\n",
            "Epoch 77/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2077\n",
            "Epoch 78/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2080\n",
            "Epoch 79/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 80/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 81/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 82/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2094\n",
            "Epoch 83/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 84/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 85/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2076\n",
            "Epoch 86/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 87/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 88/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 89/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 90/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2078\n",
            "Epoch 91/100\n",
            "3342/3342 [==============================] - 11s 3ms/step - loss: 0.2080 - val_loss: 0.2087\n",
            "Epoch 92/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2085\n",
            "Epoch 93/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 94/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 95/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2078\n",
            "Epoch 96/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 97/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2100\n",
            "Epoch 98/100\n",
            "3342/3342 [==============================] - 12s 3ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 99/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2075\n",
            "Epoch 100/100\n",
            "3342/3342 [==============================] - 12s 4ms/step - loss: 0.2079 - val_loss: 0.2078\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepSVDD(batch_size=128,\n",
              "     c=array([0.1    , 0.1    , 0.11838, 0.32417, 0.     , 0.1    , 0.1    ,\n",
              "       0.1    , 0.17974, 0.15458, 0.1    , 0.1    , 0.1    , 0.1    ,\n",
              "       0.41826, 0.12526, 0.1    , 0.1    , 0.53061, 0.1    , 0.1    ,\n",
              "       0.1    , 0.     , 0.1    , 0.1    , 0.1    , 0.5356 , 0.     ,\n",
              "       0.1    , 0....      0.25452, 0.1    , 0.1    , 0.1    , 0.1    , 0.29764, 0.18762,\n",
              "       0.1    ], dtype=float32),\n",
              "     contamination=0.1, dropout_rate=0.2, epochs=100,\n",
              "     hidden_activation='relu', hidden_neurons=[128, 64],\n",
              "     l2_regularizer=0.1, optimizer='adam', output_activation='sigmoid',\n",
              "     preprocessing=False, random_state=None, use_ae=False,\n",
              "     validation_size=0.1, verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss3 = model3.history_\n",
        "loss4 = model4.history_\n",
        "loss5 = model5.history_"
      ],
      "metadata": {
        "id": "hfQd3UJtHjNT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.ticker import MaxNLocator\n",
        "ax = plt.figure().gca()\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "\n",
        "plt.plot(np.arange(1,21), loss1['val_loss'][:20])\n",
        "plt.plot(np.arange(1,21), loss2['val_loss'][:20])\n",
        "plt.plot(np.arange(1,21), loss3['val_loss'][:20])\n",
        "plt.plot(np.arange(1,21), loss4['val_loss'][:20])\n",
        "plt.plot(np.arange(1,21), loss5['val_loss'][:20])\n",
        "plt.legend(['model1','model2','model3','model4','model5'],loc = \"upper right\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.xlabel(\"epoch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "xccwWGemHsFw",
        "outputId": "a571dc5e-cbfe-40b2-dc55-16eded5ad717"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model 2 has the lowest validation loss\n",
        "\n",
        "setting: batch_size =64, hidden_neurons= [64, 32], use_ae=False"
      ],
      "metadata": {
        "id": "7j_Elc78H9n0"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Neural Network Base Model tuning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxN2OhB4W7qHrL89D5bLuo",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}